{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the P1's HPC documentation!","text":"<p>The Pioneer Centre for Artificial Intelligence (P1 for short) is a Danish cross-university research centre. This  documentation is for the HPC which is made available for affiliates of the center. For more information regarding the centre, we refer to P1 Homepage.</p> <p>The goal of the P1 HPC is to:</p> <ul> <li>Enable research opportunities for affiliates of P1, with regards to its objectives</li> <li>Support collaborations within the center</li> </ul> <p>However, since resources are limited and we hope to serve as many P1 members as possible, large scale projects are  better supported within their own funding. The compute cluster cannot be used for commercial purposes.</p> <p>In this documentation, we cover only the basics for getting started. Since the cluster is situated within the  DTU Computing Center, the documentation of the technical side of things can be found on their  website and the P1 specific page there. When having used support from DCC  (the DTU HPC center administrating our DTU cluster) please cite the following reference:</p> <pre><code>http://doi.org/10.48714/DTU.HPC.0001\n</code></pre>"},{"location":"compute_resources/","title":"P1-Owned Compute Resources","text":"<p>The goal of the P1 HPC is to enable research (collaborations) as widely as possible within P1. However, the goal is not  to support very large projects, as this would hinder the usability for other users (we would rather support 10 smaller  projects, then 1 large one). In practical terms this means that for example training a large language or vision model  from scratch is not the intended use-case.</p>"},{"location":"compute_resources/#hierarchy-of-compute-resources","title":"Hierarchy of Compute Resources","text":"<p>Depending on what you are doing, you will need different levels of compute. The following table gives an overview of the different levels of compute and what they are suitable for.</p> Tier Organization level Description 4 Personal Personal laptop and/or desktop with either no GPUs or a single one. At this level compute is limited and mostly consist of code development and running experiment with small datasets and models. 3 Institute The next level is compute clusters containing either several smaller GPUs or a few larger once. Multiple experiments can be run in parallel and models can be parallelized over a few GPUs, but it is not feasible to train very large models from scratch. Expect limitations to wall time (a couple of days), storage (arund \u00bd-1 TB) and number of nodes/GPUs used (around 2-16). 2 National Denmark does currently not have a national HPC center with GPUs. At this level multiple experiments can be run in parallel and models can be parallelized over a few GPUs, but it is not feasible to train very large models from scratch. Expect less restrictions to wall time (1-2 weeks), storage (arund 1-10 TB) and number of nodes/GPUs used (around 16-64). 1 European At this level it is feasible to train very large models from scratch. Getting access to this level of compute is usually done by applying for access to a European HPC center. Usually easy to get 8-16 GPU's, with possibliity to get up towards 500-800 GPU's depending on the project you are doing and the particular HPC center. Walltime is usually not restricted, instead limitations are set on total number of GPU hours used. See Karolina and Meluxina for examples of European HPC centers with GPUs. 0 Global The highest level of compute. Possibility to above 1000 GPU's and doing calculations at exoscale. Usually only available to large research projects and collaborations with industry. See LUMI for an example of a global HPC center with GPUs. <p>The P1 cluster sits at level 3 in the hierarchy. The current hardware configuration consists of 7 Lenovo ThinkSystem  SR665 V3 servers, each with 2 AMD EPYC 9354 32-Core Processors, and 768GB of RAM. Each node is equipped with 2 NVIDIA  H100 PCIe GPU cards, with 80 GB of memory each. The Operating System is Alma Linux (ver. 9.2), and the servers are under  control of a scheduling environment (LSF), to manage the concurrent workload of several simultaneous users. </p> <p>5 of the servers are available for batch jobs and in a queue named <code>p1</code>. 2 of the servers are reserved for interactive  usage on a queue called <code>p1i</code>, and should be used for setting up the models, the computational environment, and for  everything that needs interactive access.</p> <p>The servers are connected to a 60TiB storage, accessible under /dtu/p1, this is reserved to P1 users.  </p>"},{"location":"compute_resources/#fair-use","title":"Fair Use","text":"<p>The following rules are in place to ensure fair use of the P1 HPC:</p> <ul> <li>maximum wall time: 72 hours</li> <li>maximum amount of nodes/GPU's used: 50% of the available resources</li> <li>500 gb of storage (+30gb in home directory)</li> </ul> <p>If you have a project that requires more resources than the above, please contact the governance group  compute-governance-p1@aicentre.dk to discuss your needs.</p> <p>Warning</p> <p>The current P1 HPC cluster is only intended for non-GDPR data e.g. public datasets, open benchmarks, etc. To be more specific, you can still work on private datasets because your home directory is not shared with other users, but  data is not encrypted in a way to be GDPR compliant. P1 is in the process of setting up a separate HPC for GDPR  data.</p>"},{"location":"compute_resources/#investing-your-compute-budget","title":"Investing Your Compute Budget","text":"<p>If you have compute funding or are applying for compute funding, you are very welcome to contact P1's compute  coordinator for a consultation.</p> <p>Generally, P1 can help purchasing hardware for you if it is to be a part of P1's clusters. In this setup you would have  priority access to the hardware that you purchase, but if it is unused other researchers at P1 can use it as well.  Most HPC hardware has an expected utilization time of approximately 5 years.</p> <p>An example of a compute budget expenditure - a researcher might have 400k DKK for non-GDPR computing. In that case P1  can purchase a 2xA100 node, with the researcher getting priority access. With the last 100k DKK, we recommend a  diversification of the researchers compute portfolio. Using some of the 100k for personal computers with powerful GPU's  and reserving a siginificant portion of funds for later. This could be for storage or cloud compute. Having the budget  to spring for cloud compute if peak pressure on other computing resources makes training a model in time for a  submission deadline infeasible can be necessary.</p>"},{"location":"contribute/","title":"Contribute","text":"<p>We welcome contributions to the documentation. Before you start, we recommend contacting one of the Github organization owners to discuss your plans. This will help us coordinate our efforts and avoid duplication of work. Additionally, this also allows us to add you to the organization, so you can work directly on the repository.</p>"},{"location":"contribute/#how-to-contribute","title":"How to contribute","text":"<p>If you are not already fimilar with the process of contributing to a Github repository, you can find a detailed guide here. If you are only going to make a small change, you can use the Github web interface to edit the file directly.</p> <p>Start by creating a new branch from the branching menu and name it after the feature you want to create </p> <p>GitHub will prompt you to create a new branch with the name of your feature. Click create branch. </p> <p>Once on the correct branch - navigate to the file you want to edit and click the edit button to go into editor mode. </p> <p>Perform your edits, this part can be performed multiple times, and with each commit make sure to put in a title that is  informative and makes sense. If the title is too long feel free to describe your changes in greater details in the  extended description. Make sure you commit to your newly created branch. </p> <p>Go back to the main repository page and ensure you are still on your own branch. Click \"Compare &amp; pull request\" to begin  the pull request and merging process. </p> <p>Set the buttons highlighted in green to \"base: main\" and \"compare: $my_branch$\". Write an overall message for your pull  request and click the \"Create pull request\"-button. </p> <p>Once all checks (if any are running) have passed you can click the \"Merge pull request\"-button or wait until an  administrator has approved your pull request. If your pull request did not pass all checks, continue working on your  changes until they do. </p>"},{"location":"getstarted/","title":"Get started","text":""},{"location":"getstarted/#getting-access","title":"Getting access","text":"<p>To get access to the cluster, one should:</p> <ol> <li> <p>First register to be a member of P1. You do this by filling out this     form. Only move on to the next step after you are correctly listed     on the P1 webpage.</p> </li> <li> <p>The cluster is hosted at DTU and the next step is therefore to get a DTU account, so that the DTU compute environment     can be accessed. This can be obtained by filling out this form https://forms.office.com/e/DG5qCfs6Wm which will     be sent to Henning Christiansen, head of DTUs compute center. Only signup using an official university email address     is accepted. He will create an account for you and contact you with the details.</p> </li> </ol>"},{"location":"getstarted/#accessing-the-cluster","title":"Accessing the cluster","text":"<p>To access the cluster outside of DTUs network please refer to the guide in the following section.</p> <p>The compute cluster is accessible at <code>login9.hpc.dtu.dk</code> via SSH, where the user will log into their home directory.  Note hat the home directories have limited storage (50gb), but more storage is available at <code>/dtu/p1/</code>.</p> <p>We have 1 interactive node that you can use for installing packages and try-runs of your experiments. Please do not run  any heavy jobs on the login node, and use batch jobs for your experiments.</p>"},{"location":"getstarted/#accessing-the-cluster-outside-of-dtu","title":"Accessing the cluster outside of DTU","text":"<p>This section describes one possible option to access the cluster from outside of DTUs network using a Linux machine.  The guide assumes that you have obtained a DTU account beforehand.</p>"},{"location":"getstarted/#first-access-steps","title":"First access steps","text":"<ol> <li>Download Cisco AnyConnect VPN client</li> <li>Go to https://dtubasen.dtu.dk and choose sign-in via Azure multi-factor auth. using your full DTU username      (including <code>@dtu.dk</code>)</li> <li>Set up a new multi-factor auth to DTU e.g via Authenticator app.</li> <li>Establish connection to vpn.dtu.dk in the AnyConnect client.</li> <li>Log in to the cluster via SSH at <code>login9.hpc.dtu.dk</code> using your DTU credentials (username without <code>@dtu.dk</code>).</li> <li>Close connection to the cluster.</li> </ol> <p>You can choose to keep using VPN to access the cluster or use SSH keys + passwords instead. For the latter please  follow the steps below.</p>"},{"location":"getstarted/#access-with-ssh-keys-passwords","title":"Access with SSH keys + passwords","text":"<ol> <li>Ensure that you have completed the steps described above to establish access via VPN.</li> <li>Generate an SSH key using ssh-keygen.</li> <li>Copy the key to the server e.g. using: <code>ssh-copy-id -i ~/.ssh/keyname username@login9.hpc.dtu.dk</code></li> <li>Log in to the cluster: <code>ssh username@login9.hpc.dtu.dk</code></li> </ol> <p>For more technical information please refer to the  P1 compute cluster documentation at DTU DCC.</p>"},{"location":"learning_resources/","title":"Learning Resources","text":"<p>In this section you will find a variety of learning resources in topics such as machine learning operations, machine  learning systems, performance optimization and HPC systems. For course and training offered by the DTU HPC team, please visit this page</p>"},{"location":"learning_resources/#intro-talks","title":"Intro talks","text":"Date Title Speaker Video 27/09/22 Introduction to GPUs Hans Henrik Brandenborg S\u00f8rensen 270922Video 30/11/22 Introduction to HPC Systems Bernd Dammann 301122Video"},{"location":"learning_resources/#learning-resources_1","title":"Learning resources","text":"Name Description Type Affiliation MLSys Seminar Series Online videos of MLSys talks Seminar Series Stanford"},{"location":"support/","title":"Support","text":"<p>In general the P1 does not have official support staff. Instead we rely on the community to help each other. However, for questions regarding the cluster, we have the following two contact e-mails:</p> <ul> <li> <p>For technical issues, like broken or missing packages, contact support@hpc.dtu.dk </p> </li> <li> <p>For policy issues, like priority access, queuing system etc. contact compute-governance-p1@aicentre.dk</p> </li> </ul> <p>If your submission to join the cluster has been pending for more than a week, you can contact the member of the P1 HPC  committee of your institution. General announcments regarding the cluster will be made in the <code>#compute</code> channel on the  official P1-slack. Simiarly if you have general questions regarding compute, including using the P1-cluster, feel free  to post those questions in the <code>#ask-compute</code> channel.</p> Institution Name E-mail DTU Nicki Skafte Detlefsen nsde@dtu.dk ITU Rob van der Goot robv@itu.dk KU Mikkel Fruelund Odgaard miod@di.ku.dk AAU Andreas Aakerberg anaa@create.aau.dk AU Akhil Arora akhil.arora@cs.au.dk"}]}