{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the P1's HPC documentation!","text":"<p>The Pioneer Centre for Artificial Intelligence (P1 for short) is a Danish cross-university research centre. This documentation provides a concise overview for P1 affiliates on using our own HPC facilities and relevant partners.</p> System Tier Price (compute) Price (storage) GDPR data P1 DTU 3 Free to P1 users Free to P1 users No P1 NGC 3 Free to P1 users Free to P1 users Yes Gefion (Partner) 1/2 Pay-per-hour Pay-per-month No (not yet) <p>For additional details on P1, visit the P1 Homepage and P1 Computing.</p> <p>The goal of the P1 HPCs is to enable research opportunities for affiliates of P1, supporting projects of various scales. While we prioritize smaller projects as they represent the majority of research needs, we also support larger projects through our tiered compute resources, including the capability to train very large models and support some European-level workloads on Tier 2/1 facilities.</p> <p>However, since resources are limited and we aim to serve as many P1 members as possible, very large scale projects might be better supported within their own funding. The compute clusters are primarily intended for research projects. While commercial use is generally not permitted on the P1 DTU and NGC HPCs, the Gefion HPC may accommodate certain commercial research projects on a case-by-case basis, subject to approval and specific terms.</p> <p>In this documentation, we cover only a broad overview of the available HPC facilities and basics for getting started. Since the clusters are installed at separate institutions, the documentation on the technical side of things differ by facility:</p> <ul> <li>P1 DTU HPC - Technical Documentation</li> <li>P1 DTU HPC - Support</li> </ul> <p>When having used support from DCC (the DTU HPC center administrating our DTU cluster) please cite the following reference:</p> <pre><code>http://doi.org/10.48714/DTU.HPC.0001\n</code></pre>"},{"location":"01_p1_dtu_hpc/","title":"P1 DTU HPC","text":""},{"location":"01_p1_dtu_hpc/#overview","title":"Overview","text":"<p>The P1 DTU HPC is hosted at DTU and provides high-performance computing resources for P1 members. It is particularly suitable for medium to large-scale machine learning experiments and research projects.</p>"},{"location":"01_p1_dtu_hpc/#getting-access","title":"Getting Access","text":"<p>Before accessing the P1 DTU HPC, you must first register to become a member of P1 by filling out this form.</p>"},{"location":"01_p1_dtu_hpc/#account-setup","title":"Account Setup","text":"<p>Fill out the DTU account request form: https://forms.office.com/e/DG5qCfs6Wm. Only signup using an official university email address is accepted.</p> <p>The form will be processed by Henning Christiansen, head of DTU's compute center and you will receive account details via email once created.</p>"},{"location":"01_p1_dtu_hpc/#accessing-the-cluster","title":"Accessing the Cluster","text":"<p>The compute cluster is accessible at <code>login9.hpc.dtu.dk</code> via SSH. Note that:</p> <ul> <li>Home directories have limited storage (30GB)</li> <li>Additional storage is available at <code>/dtu/p1/</code></li> <li>Interactive node is available for package installation and test runs</li> <li>Heavy jobs should be submitted as batch jobs</li> </ul>"},{"location":"01_p1_dtu_hpc/#access-from-outside-dtu-network","title":"Access from Outside DTU Network","text":"<ol> <li>Download Cisco AnyConnect VPN client (see OpenConnect for Linux)</li> <li>Go to https://dtubasen.dtu.dk and sign in via Azure multi-factor auth using your full DTU username</li> <li>Set up multi-factor authentication</li> <li>Connect to vpn.dtu.dk using AnyConnect</li> <li>SSH to <code>login9.hpc.dtu.dk</code> using your DTU credentials</li> </ol> <p>For persistent access, you can set up SSH keys:</p> <pre><code># 1. Generate key\nssh-keygen -t ed25519 -f ~/.ssh/keyname\n\n# 2. Copy public key\nssh-copy-id -i ~/.ssh/keyname.pub username@login9.hpc.dtu.dk\n\n# 3. Connect\nssh -i ~/.ssh/keyname username@login9.hpc.dtu.dk\n</code></pre>"},{"location":"01_p1_dtu_hpc/#support","title":"Support","text":"<ul> <li>Technical Support: For issues like broken or missing packages, contact support@hpc.dtu.dk</li> <li>Policy Support: For policy issues, contact compute-governance-p1@aicentre.dk</li> <li>General Questions: Use the <code>#compute</code> or <code>#ask-compute</code> channels on P1 Slack</li> <li>Compute Coordinator: Contact bstja@dtu.dk for general or technical compute-related questions</li> </ul> <p>For more technical information, refer to the P1 compute cluster documentation at DTU DCC. </p>"},{"location":"01_p1_dtu_hpc/#fair-use-policy","title":"Fair Use Policy","text":"<p>The following rules are in place to ensure fair use of the P1 DTU HPC:</p> <ul> <li>Maximum wall time: 72 hours</li> <li>Maximum number of GPUs in a job: 2 (one node)</li> <li>Maximum running jobs: ~50% of total available GPUs</li> <li>500 gb of storage (+30gb in home directory)</li> </ul> <p>If you have a project that requires more storage resources than the above, please contact the governance group compute-governance-p1@aicentre.dk to discuss your needs.</p> <p>Warning</p> <p>The P1 DTU HPC is only intended for non-GDPR data e.g. public datasets, open benchmarks, etc. To be more specific, you can still work on private datasets because your home directory is not shared with other users, but data is not encrypted in a way to be GDPR compliant. For GDPR-compliant data processing, please use the P1 NGC HPC.</p>"},{"location":"01_p1_dtu_hpc/#investing-your-compute-budget","title":"Investing Your Compute Budget","text":"<p>If you have compute funding or are applying for compute funding, you are very welcome to contact P1's compute coordinator for a consultation.</p> <p>Generally, P1 can help purchasing hardware for you if it is to be a part of P1's clusters. In this setup you would have priority access to the hardware that you purchase, but if it is unused other researchers at P1 can use it as well. Most HPC hardware has an expected utilization time of approximately 5 years.</p> <p>An example of a compute budget expenditure - a researcher might have 400k DKK for non-GDPR computing. In that case P1 can purchase a 2xA100 node, with the researcher getting priority access. With the last 100k DKK, we recommend a diversification of the researchers compute portfolio.</p> <p>Using some of the 100k for personal computers with powerful GPU's and reserving a significant portion of funds for later. This could be for storage or cloud compute. Having the budget to spring for cloud compute if peak pressure on other computing resources makes training a model in time for a submission deadline infeasible can be necessary.</p>"},{"location":"01_p1_dtu_hpc/#hardware-specifications","title":"Hardware Specifications","text":"<ul> <li>7 Lenovo ThinkSystem SR665 V3 servers</li> <li>Each node specifications:<ul> <li>2 AMD EPYC 9354 32-Core Processors</li> <li>768GB RAM</li> <li>2 NVIDIA H100 PCIe GPUs (80GB each)</li> </ul> </li> <li>Storage: 60TiB shared storage</li> <li>Operating System: Alma Linux</li> <li>Scheduling Environment: LSF</li> <li>Resource Allocation:<ul> <li>7 nodes available for batch jobs (queue: <code>p1</code>)</li> <li>1 node reserved for interactive usage (queue: <code>p1i</code>)</li> </ul> </li> </ul>"},{"location":"02_p1_ngc_hpc/","title":"P1 NGC HPC","text":"<p>Waiting List</p> <p>We are currently in the process of tackling some requirements related to the setup of data processing agreements. We are not onboarding new users until this is in place. You can still sign up now to be placed on the waiting list, and we'll keep you updated on the progress.</p>"},{"location":"02_p1_ngc_hpc/#overview","title":"Overview","text":"<p>The P1 NGC HPC is hosted at the National Genome Centre and is designed for secure data processing with GDPR compliance. It provides a secure environment for handling sensitive data and research projects.</p>"},{"location":"02_p1_ngc_hpc/#getting-access","title":"Getting Access","text":"<p>Before accessing the P1 NGC HPC, you must first register to become a member of P1 by filling out this form.</p> <p>Each project needs to bring a record (a signed Data Processing Agreement should do) that explicitly mentions NGC as a data processor and that the data is allowed to be stored there.</p> <ul> <li>If the project poses a high risk to individuals whose personal data is being processed a Data Protection Impact Assessment (DPIA) will be needed also. </li> </ul>"},{"location":"02_p1_ngc_hpc/#account-setup","title":"Account Setup","text":"<p>Complete and sign the NGC user creation form and forward it to the Compute Coordinator to request access.</p> <ul> <li>You will be added to the NGC slack channel once you gain access.</li> </ul>"},{"location":"02_p1_ngc_hpc/#accessing-the-cluster","title":"Accessing the Cluster","text":"<p>The P1 NGC HPC is an air-gapped system requiring:</p> <ul> <li>Multi-factor authentication (MFA)</li> <li>A client for accessing the remote VM entrypoint</li> <li>Specific access instructions will will follow after registration. But you can expect to use SFTP for the transferring if data into the system.</li> </ul>"},{"location":"02_p1_ngc_hpc/#support","title":"Support","text":"<ul> <li>Technical Support: For technical issues, contact support@ngc.dk</li> <li>Policy Support: For policy issues, contact compute-governance-p1@aicentre.dk</li> <li>General Questions: Use the <code>#compute</code> or <code>#ask-compute</code> channels on P1 Slack</li> <li>Compute Coordinator: Contact bstja@dtu.dk for general or technical compute-related questions</li> </ul> <p>For questions about pending access requests or general compute-related inquiries, you can also contact your institution's P1 HPC Committee Member. </p>"},{"location":"02_p1_ngc_hpc/#hardware-specifications","title":"Hardware Specifications","text":"<ul> <li>Air-gapped system for secure data processing</li> <li>GDPR compliant infrastructure</li> <li>Secure storage solutions</li> <li>Specific hardware details available upon access approval</li> <li>Scheduling Environment: SLURM</li> <li>Resource allocation details provided during onboarding</li> </ul>"},{"location":"03_p1_gefion_hpc/","title":"Gefion HPC (Partner)","text":"<p>Not Free, Waiting List</p> <p>Gefion is not free. P1 might be able to cover some expenses for a test run, but it's expected that researchers at this point will bring their own funding. DCAI and Novo Nordisk might publish grant applications that provide GPU hours through vouchers. We are currently working out the onboarding details. You can still sign up now to be placed on the waiting list, and we'll keep you updated on the progress.</p>"},{"location":"03_p1_gefion_hpc/#overview","title":"Overview","text":"<p>The Gefion HPC is a national-level facility that can support both Tier 2 and some Tier 1 workloads. It is particularly suitable for large-scale model training and distributed computing tasks.</p>"},{"location":"03_p1_gefion_hpc/#getting-access","title":"Getting Access","text":"<p>Before accessing the Gefion HPC, you must first register to become a member of P1 by filling out this form.</p>"},{"location":"03_p1_gefion_hpc/#account-setup","title":"Account Setup","text":"<p>Contact the Compute Coordinator to request access to the Gefion HPC.</p>"},{"location":"03_p1_gefion_hpc/#accessing-the-cluster","title":"Accessing the Cluster","text":"<p>Access guidelines and procedures are currently being developed. Detailed instructions will be provided to approved users.</p>"},{"location":"03_p1_gefion_hpc/#support","title":"Support","text":"<ul> <li>Technical Support: For technical issues, contact the Compute Coordinator</li> <li>Policy Support: For policy issues, contact compute-governance-p1@aicentre.dk</li> <li>General Questions: Use the <code>#compute</code> or <code>#ask-compute</code> channels on P1 Slack</li> <li>Compute Coordinator: Contact bstja@dtu.dk for general or technical compute-related questions</li> </ul> <p>For questions about pending access requests or general compute-related inquiries, you can also contact your institution's P1 HPC Committee Member. </p>"},{"location":"03_p1_gefion_hpc/#hardware-specifications","title":"Hardware Specifications","text":"<ul> <li>NVIDIA DGX SuperPOD</li> <li>Multiple DGX nodes (8 x H100s) with high-performance interconnects</li> <li>Large-scale GPU resources for distributed training</li> <li>High-bandwidth storage solutions</li> <li>Scheduling Environment: SLURM</li> </ul>"},{"location":"04_learning_resources/","title":"Learning Resources","text":"<p>In this section you will find a variety of learning resources in topics such as machine learning operations, machine learning systems, performance optimization and HPC systems. For course and training offered by the DTU HPC team, please visit this page</p>"},{"location":"04_learning_resources/#intro-talks","title":"Intro talks","text":"Date Title Speaker Video 27/09/22 Introduction to GPUs Hans Henrik Brandenborg S\u00f8rensen 270922Video 30/11/22 Introduction to HPC Systems Bernd Dammann 301122Video"},{"location":"04_learning_resources/#learning-resources_1","title":"Learning resources","text":"Name Description Type Affiliation MLSys Seminar Series Online videos of MLSys talks Seminar Series Stanford"},{"location":"05_contribute/","title":"Contribute","text":"<p>We welcome contributions to the documentation. Before you start, we recommend contacting one of the Github organization owners to discuss your plans. This will help us coordinate our efforts and avoid duplication of work.</p> <p>Additionally, this also allows us to add you to the organization, so you can work directly on the repository.</p>"},{"location":"05_contribute/#how-to-contribute","title":"How to contribute","text":"<p>If you are not already familiar with the process of contributing to a Github repository, you can find a detailed guide here. If you are only going to make a small change, you can use the Github web interface to edit the file directly.</p> <p>Start by creating a new branch from the branching menu and name it after the feature you want to create </p> <p>GitHub will prompt you to create a new branch with the name of your feature. Click create branch. </p> <p>Once on the correct branch - navigate to the file you want to edit and click the edit button to go into editor mode. </p> <p>Perform your edits, this part can be performed multiple times, and with each commit make sure to put in a title that is informative and makes sense. If the title is too long feel free to describe your changes in greater details in the extended description. Make sure you commit to your newly created branch. </p> <p>Go back to the main repository page and ensure you are still on your own branch. Click \"Compare &amp; pull request\" to begin the pull request and merging process. </p> <p>Set the buttons highlighted in green to \"base: main\" and \"compare: $my_branch$\". Write an overall message for your pull request and click the \"Create pull request\"-button. </p> <p>Once all checks (if any are running) have passed you can click the \"Merge pull request\"-button or wait until an administrator has approved your pull request. If your pull request did not pass all checks, continue working on your changes until they do. </p>"},{"location":"about/","title":"Available Compute Resources","text":"<p>The goal of the P1 HPCs is to enable research (collaborations) as widely as possible within P1, supporting projects of various scales. While we prioritize smaller projects as they represent the majority of research needs, we also support larger projects through our tiered compute resources.</p> <p>In practical terms this means that while training a large language or vision model from scratch might not be suitable for our Tier 3 facilities, such projects can be accommodated on a Tier 2 national facility (ex. Gefion HPC), which is specifically designed for large-scale model training and can even support some European-level (Tier 1) workloads.</p>"},{"location":"about/#support","title":"Support","text":"<p>In general the P1 does not have official support staff. Instead we rely on the community to help each other. However, for questions regarding the cluster, we have the following two contact e-mails:</p> <ul> <li>For technical issues, like broken or missing packages, contact support@hpc.dtu.dk </li> <li>For policy issues, like priority access, queuing system etc. contact compute-governance-p1@aicentre.dk</li> </ul> <p>If your submission to join the cluster has been pending for more than a week, you can contact the member of the P1 HPC  committee of your institution. General announcments regarding the cluster will be made in the <code>#compute</code> channel on the  official P1-slack. Simiarly if you have general questions regarding compute, including using the P1-cluster, feel free  to post those questions in the <code>#ask-compute</code> channel.</p> Institution Name E-mail DTU Nicki Skafte Detlefsen nsde@dtu.dk ITU Rob van der Goot robv@itu.dk KU Mikkel Fruelund Odgaard miod@di.ku.dk AAU Andreas Aakerberg anaa@create.aau.dk AU Akhil Arora akhil.arora@cs.au.dk"},{"location":"about/#hierarchy-of-compute-resources","title":"Hierarchy of Compute Resources","text":"<p>Depending on what you are doing, you will need different levels of compute. The following table gives an overview of the different levels of compute and what they are suitable for.</p> Tier Organization level Description 4 Personal Personal laptop and/or desktop with either no GPUs or a single one. At this level compute is limited and mostly consist of code development and running experiment with small datasets and models. 3 Institute The next level is compute clusters containing either several smaller GPUs or a few larger once. Multiple experiments can be run in parallel and models can be parallelized over a few GPUs, but it is not feasible to train very large models from scratch. Expect limitations to wall time (a couple of days), storage (arund \u00bd-1 TB) and number of nodes/GPUs used (around 2-16). 2 National National HPC facilities provide the necessary resources for large-scale model training. Expect less restrictions to wall time (1-2 weeks), storage (around 1-10 TB) and number of nodes/GPUs used (around 16-64). Some national facilities can also support European-level (Tier 1) workloads. For example, P1's Gefion HPC is a national facility that can support both Tier 2 and possibly Tier 1 workloads. 1 European European HPC centers offer significantly larger scale compute resources, typically providing easy access to 8-16 GPUs with the possibility of scaling up to 500-800 GPUs depending on project requirements and the specific HPC center. Unlike lower tiers, walltime is usually not restricted; instead, limitations are set on total GPU hours used. See Karolina and Meluxina for examples of European HPC centers with GPUs. 0 Global The highest level of compute. Possibility to above 1000 GPU's and doing calculations at exoscale. Usually only available to large research projects and collaborations with industry. See LUMI for an example of a global HPC center with GPUs. <p>The P1 DTU and NGC HPCs sit at level 3 in the hierarchy, while Gefion HPC primarily operates at level 2 but can support some level 1 workloads, making it capable of supporting both national and some European-level research projects.</p>"}]}