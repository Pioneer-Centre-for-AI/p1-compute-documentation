---
title: About P1 Compute Resources
description: Learn more about the P1AI's HPC resources.
---

import People from '../../components/People.astro';
import { Badge } from '@astrojs/starlight/components';

The **Pioneer Centre for Artificial Intelligence** (P1) is a Danish cross-university research centre and consortium dedicated to advancing AI research through collaboration with partnering institutions.

This documentation aims to centralize practical knowledge and provide a concise guide for P1 affiliates on how to access and use our available high-performance computing (HPC) facilities.

![Alt Text](../../assets/PCAI_HQ.avif)

A central mission of P1 is to foster broad research collaboration, both within the centre and across its affiliated universities. Our HPC resources are designed to support P1 researchers at partner institutions, enabling projects of varying scale and complexity.

For more information, visit the [P1 Homepage](https://www.aicentre.dk/) or [P1 Computing](https://www.aicentre.dk/computing).

# Available Compute Resources

While we prioritize smaller projects as they represent the majority of research needs, we also support larger projects through our tiered compute resources, including the capability to train very large models and support European-level workloads on Tier 2/1 facilities. But, very large scale projects might be better supported within their own funding.

In practical terms this means that while training a large language or vision model from scratch might not be suitable for our Tier 3 facilities, such projects can be accommodated on a Tier 2 national facility (ex. Gefion HPC), which is specifically designed for large-scale model training and can even support some European-level (Tier 1) workloads.

## Support

In general the P1 does not have official support staff. Instead we rely on the community to help each other. However, for questions regarding the cluster, we have the following two contact e-mails:

* For technical issues, like broken or missing packages, contact [support@hpc.dtu.dk](mailto:support@hpc.dtu.dk)
* For policy issues, like priority access, queuing system etc. contact [compute-governance-p1@aicentre.dk](mailto:compute-governance-p1@aicentre.dk)

If your submission to join the cluster has been pending for more than a week, you can contact the member of the P1 HPC
governance board of your institution. General announcements regarding the cluster will be made in the <a href="https://pioneercentreforai.slack.com/channels/compute" target="_blank" rel="noopener noreferrer">#compute</a> channel on the
official P1-slack. Similarly if you have general questions regarding compute, including using the P1-cluster, feel free
to post those questions in the <a href="https://pioneercentreforai.slack.com/channels/ask-compute" target="_blank" rel="noopener noreferrer">#ask-compute</a> channel.

<People
  heading="HPC Governance Board"
  subheading="Contact Information"
  description=""
  people={[
    {
      name: "Nicki Skafte Detlefsen",
      role: "DTU",
      institution: "Technical University of Denmark",
      email: "nsde@dtu.dk",
      profileUrl: "https://orbit.dtu.dk/en/persons/nicki-skafte-detlefsen"
    },
    {
      name: "Rob van der Goot",
      role: "ITU",
      institution: "IT University of Copenhagen",
      email: "robv@itu.dk",
      profileUrl: "https://pure.itu.dk/en/persons/rob-van-der-goot"
    },
    {
      name: "Mikkel Fruelund Odgaard",
      role: "KU",
      institution: "University of Copenhagen",
      email: "miod@di.ku.dk",
      profileUrl: "https://di.ku.dk/ansatte/?pure=da/persons/770606"
    },
    {
      name: "Andreas Aakerberg",
      role: "AAU",
      institution: "Aalborg University",
      email: "anaa@create.aau.dk",
      profileUrl: "https://vbn.aau.dk/da/persons/132677"
    },
    {
      name: "Akhil Arora",
      role: "AU",
      institution: "Aarhus University",
      email: "akhil.arora@cs.au.dk",
      profileUrl: "https://www.au.dk/en/akhil.arora@cs.au.dk"
    }
  ]}
/>

## Investing Your Compute Budget

Generally, P1 can help purchasing hardware for you if it is to be a part of P1's clusters. In this setup you would have priority access to the hardware that you purchase, but if it is unused other researchers at P1 can use it as well. Most HPC hardware has an expected utilization time of approximately 5 years.

An example of a compute budget expenditure:

> A researcher might have 400k DKK for non-GDPR computing. In that case P1 can purchase a 2xA100 node, with the researcher getting priority access. With the last 100k DKK, we recommend a diversification of the researchers compute portfolio.

> Using some of the 100k for personal computers with powerful GPU's
and reserving a significant portion of funds for later. This could be for storage or cloud compute. Having the budget to spring for cloud compute if peak pressure on other computing resources makes training a model in time for a submission deadline infeasible can be necessary.

{/* ## Personal Machines <Badge text="TO BE DISCUSSED" variant="caution" />

Convenience and flexibility is important when doing computational research, and having a personal machine with a powerful GPU can be a great asset. To quickly test ideas, run small experiments and develop code without waiting for access to shared resources.

But who should be allowed to have a personal machine? Someone should vouch for the researcher, and they should have a good reason as well as being very adept at using such a machine, since this will fall outside the shared compute resources. How do we ensure that the researcher falls into this category? It will also require more micro management..

Could we enable these machines to be decentralized compute resources? This would allow the researcher to use their personal machine for larger experiments, but also allow others to use it when it is not in use. This would require a lot of trust in the researcher, and a lot of work to set up.

How do we avoid users hoarding these resources? Getting a personal machine is a privilege, not a right. So we need to make sure that you actually need it.

Do we even have an overview of all the personal machines that P1 offers access to? */}

## Hierarchy of Compute Resources

Depending on what you are doing, you will need different levels of compute. The following table gives an overview of the different levels of compute and what they are suitable for.

Tier | Organization level | Description
---- | ------------------ | -----------
4    | Personal           | Personal laptop and/or desktop with either no GPUs or a single one. At this level compute is limited and mostly consists of code development and running experiments with small datasets and models.
3    | Institute          | The next level is compute clusters containing either several smaller GPUs or a few larger ones. Multiple experiments can be run in parallel and models can be parallelized over a few GPUs, but it is not feasible to train very large models from scratch. Expect limitations to wall time (a couple of days), storage (around Â½-1 TB) and number of nodes/GPUs used (around 2-16).
2    | National           | National HPC facilities provide the necessary resources for large-scale model training. Expect fewer restrictions to wall time (1-2 weeks), storage (around 1-10 TB) and number of nodes/GPUs used (around 16-64). Some national facilities can also support European-level (Tier 1) workloads. For example, P1's Gefion HPC is a national facility that can support both Tier 2 and possibly Tier 1 workloads.
1    | European           | European HPC centers offer significantly larger scale compute resources, typically providing easy access to 8-16 GPUs with the possibility of scaling up to 500-800 GPUs depending on project requirements and the specific HPC center. Unlike lower tiers, walltime is usually not restricted; instead, limitations are set on total GPU hours used. See [Karolina](https://eurohpc-ju.europa.eu/supercomputers/our-supercomputers_en#karolina) and [Meluxina](https://eurohpc-ju.europa.eu/supercomputers/our-supercomputers_en#meluxina) for examples of European HPC centers with GPUs.
0    | Global             | The highest level of compute. Possibility to use above 1000 GPUs and perform calculations at exascale. Usually only available to large research projects and collaborations with industry. See [LUMI](https://eurohpc-ju.europa.eu/supercomputers/our-supercomputers_en#lumi) for an example of a global HPC center with GPUs.

The P1 DTU and NGC HPCs sit at level 3 in the hierarchy, while Gefion HPC primarily operates at level 2 but can support some level 1 workloads, making it capable of supporting both national and some European-level research projects.
